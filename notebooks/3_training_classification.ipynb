{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import pytorch_lightning\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from attribute_extraction.models.attribute_classification import MultiAttributeClassifier\n",
    "from attribute_extraction.models.mapper import Mapper\n",
    "from attribute_extraction.models.train_utils import (\n",
    "    AttributeDataset,\n",
    "    MultiAttributeBatchSampler,\n",
    "    build_callbacks,\n",
    "    collate_fun_generator,\n",
    ")\n",
    "from attribute_extraction.utils.data_balancing_utils import add_weight_for_data_balancing\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setting up variables and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_code_col = \"attribute_code\"\n",
    "attribute_lov_col = \"lov_code\"\n",
    "context_col = \"context\"\n",
    "weight_col = \"weight\"\n",
    "mapped_lov_col = \"mapped_lov_code\"\n",
    "\n",
    "local_path = \"../outputs_train_workflow/\"\n",
    "\n",
    "model_name = \"distilbert-base-multilingual-cased\"\n",
    "experiment_description = \"Multi-task small GRU model with multilanguage distilledBert tokenizer\"\n",
    "num_epoch = 20\n",
    "max_len = 512\n",
    "batch_size = 128 \n",
    "freeze_backbone = True\n",
    "learning_rate = 1e-4\n",
    "projection_dim = 256\n",
    "dropout = 0.2\n",
    "data_balance = True\n",
    "upper_qn = 0.9\n",
    "lower_qn = 0.1\n",
    "lov_attribute_codes = [\"02419\", \"01746\", \"00562\", \"15344\", \"99999\"]\n",
    "train_set_uri = \"../data/train_formatted.csv\"\n",
    "val_set_uri = \"../data/val_formatted.csv\"\n",
    "test_set_uri = \"../data/train_formatted.csv\"\n",
    "\n",
    "# Save the hyper_parameters\n",
    "hyper_parameters = {\n",
    "    \"model_name\": model_name,\n",
    "    \"experiment_description\": experiment_description,\n",
    "    \"num_epoch\": num_epoch,\n",
    "    \"max_len\": max_len,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"freeze_backbone\": freeze_backbone,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"projection_dim\": projection_dim,\n",
    "    \"dropout\": dropout,\n",
    "    \"data_balance\": data_balance,\n",
    "    \"upper_qn\": upper_qn,\n",
    "    \"lower_qn\": lower_qn,\n",
    "    \"lov_attribute_codes\": lov_attribute_codes,\n",
    "    \"train_set_uri\": train_set_uri,\n",
    "    \"val_set_uri\": val_set_uri,\n",
    "    \"test_set_uri\": test_set_uri,\n",
    "\n",
    "}\n",
    "\n",
    "with open(f\"{local_path}hyper_parameters.json\", \"w\") as f:\n",
    "    json.dump(hyper_parameters, f, indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(train_set_uri).reset_index(drop=True)\n",
    "data_val = pd.read_csv(val_set_uri).reset_index(drop=True)\n",
    "data_test = pd.read_csv(test_set_uri).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['attribute_code'] = data_train['attribute_code'].apply(lambda x: str(x).zfill(5))\n",
    "data_train['lov_code'] = data_train['lov_code'].apply(lambda x: str(x).zfill(5))\n",
    "\n",
    "data_val['attribute_code'] = data_val['attribute_code'].apply(lambda x: str(x).zfill(5))\n",
    "data_val['lov_code'] = data_val['lov_code'].apply(lambda x: str(x).zfill(5))\n",
    "\n",
    "data_test['attribute_code'] = data_test['attribute_code'].apply(lambda x: str(x).zfill(5))\n",
    "data_test['lov_code'] = data_test['lov_code'].apply(lambda x: str(x).zfill(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Facile d'entretien tapis de coton \\xa0 \\xa0Ce coton naturel Flachflorteppich 100% obtient son look spécial par le motif de rayures élégantes dans des couleurs harmonieuses et douces pompons sur les coins. La conception à haut contraste fixe habilement le ton dans la chambre et donne à chaque dispositif a un facteur de confort. Créer une ambiance harmonieuse conçue combinée avec tampon de laine ou un coussin de laine qui respire le confort. \\xa0Le matériau de laine dense, doux, grâce aux propriétés d'isolation thermique, même sur un carrelage froid de sol agréablement doux et le réchauffement. \\xa0 \\xa0Vous recevez ce tapis en coton comme une sorte unique. En raison du matériau naturel et la production de tapis tissés à la main pas la même chose. En forme, la couleur et motif, il peut y avoir des différences allant jusqu'à 30% de l'image et d'autres tapis de cette qualité.\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.description_clean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train[context_col] = data_train.title + \" \" + data_train.description_clean\n",
    "data_val[context_col] = data_val.title + \" \" + data_val.description_clean\n",
    "data_test[context_col] = data_test.title + \" \" + data_test.description_clean"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = Mapper(attribute_code_col=attribute_code_col, attribute_value_col=attribute_lov_col)\n",
    "\n",
    "mapper.fit(pd.concat([data_train, data_val, data_test]))\n",
    "\n",
    "mapper.save(f\"{local_path}mapper.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = mapper.map_dataframe(data_train, mapped_col_name=mapped_lov_col)\n",
    "data_val = mapper.map_dataframe(data_val, mapped_col_name=mapped_lov_col)\n",
    "data_test = mapper.map_dataframe(data_test, mapped_col_name=mapped_lov_col)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_balance:\n",
    "\n",
    "    data_train = add_weight_for_data_balancing(\n",
    "        df=data_train,\n",
    "        label_col=mapped_lov_col,\n",
    "        weight_col=weight_col,\n",
    "        attribute_code_col=attribute_code_col,\n",
    "        upper_qn=upper_qn,\n",
    "        lower_qn=lower_qn,\n",
    "    )\n",
    "\n",
    "    data_val[weight_col] = 1\n",
    "    data_test[weight_col] = 1\n",
    "else:\n",
    "    data_train[weight_col] = 1\n",
    "    data_val[weight_col] = 1\n",
    "    data_test[weight_col] = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = MultiAttributeClassifier(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    class_config=mapper.mappings,\n",
    "    freeze_backbone=freeze_backbone,\n",
    "    warmup_steps=data_train.shape[0] // batch_size // 100,\n",
    "    estimated_stepping_batches=data_train.shape[0]\n",
    "    * num_epoch\n",
    "    // batch_size,\n",
    "    num_cycles=num_epoch,\n",
    "    projection_dim=projection_dim,\n",
    "    dropout=dropout,\n",
    "    learning_rate=learning_rate,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloaders Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = AttributeDataset(\n",
    "    data=data_train,\n",
    "    context_col_name=context_col,\n",
    "    label_col_name=mapped_lov_col,\n",
    "    attribute_code_col_name=attribute_code_col,\n",
    ")\n",
    "validation = AttributeDataset(\n",
    "    data=data_val,\n",
    "    context_col_name=context_col,\n",
    "    label_col_name=mapped_lov_col,\n",
    "    attribute_code_col_name=attribute_code_col,\n",
    ")\n",
    "test = AttributeDataset(\n",
    "    data=data_test,\n",
    "    context_col_name=context_col,\n",
    "    label_col_name=mapped_lov_col,\n",
    "    attribute_code_col_name=attribute_code_col,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train,\n",
    "    batch_sampler=MultiAttributeBatchSampler(\n",
    "        data=data_train,\n",
    "        batch_size=batch_size,\n",
    "        split_col=attribute_code_col,\n",
    "        weight_col=weight_col,\n",
    "    ),\n",
    "    collate_fn=partial(\n",
    "        collate_fun_generator,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len,\n",
    "    ),\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "validation_loader = DataLoader(\n",
    "    dataset=validation,\n",
    "    batch_sampler=MultiAttributeBatchSampler(\n",
    "        data=data_val,\n",
    "        batch_size=batch_size,\n",
    "        split_col=attribute_code_col,\n",
    "        weight_col=weight_col,\n",
    "    ),\n",
    "    collate_fn=partial(\n",
    "        collate_fun_generator,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len,\n",
    "    ),\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test,\n",
    "    batch_sampler=MultiAttributeBatchSampler(\n",
    "        data=data_test,\n",
    "        batch_size=batch_size,\n",
    "        split_col=attribute_code_col,\n",
    "        weight_col=weight_col,\n",
    "    ),\n",
    "    collate_fn=partial(\n",
    "        collate_fun_generator,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len,\n",
    "    ),\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks, metric_logger = build_callbacks(output_path=Path(local_path), model_name=\"model\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: None, using: 0 TPU cores\n",
      "/home/20014946/anaconda3/envs/pem-entities/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Missing logger folder: ../outputs_train_workflow/model\n",
      "\n",
      "  | Name                 | Type             | Params\n",
      "----------------------------------------------------------\n",
      "0 | encoder              | Embedding        | 30.6 M\n",
      "1 | classification_heads | ModuleDict       | 2.9 M \n",
      "2 | ce_loss              | CrossEntropyLoss | 0     \n",
      "----------------------------------------------------------\n",
      "33.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "33.5 M    Total params\n",
      "133.906   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c91e6df011e642cbb8b8a5f0c90dd685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c05bde98181040cbafe9a4de298aab29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "MisconfigurationException",
     "evalue": "ModelCheckpoint(monitor='validation_loss') not found in the returned metrics: ['train_loss', 'train_accuracy_02419', 'train_accuracy_99999', 'train_accuracy_00562', 'train_accuracy_15344', 'train_accuracy_01746']. HINT: Did you call self.log('validation_loss', tensor) in the LightningModule?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/pem-entities/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:644\u001b[0m, in \u001b[0;36mTrainer.run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    643\u001b[0m     \u001b[39m# run train epoch\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_loop\u001b[39m.\u001b[39;49mrun_training_epoch()\n\u001b[1;32m    646\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_steps \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_steps \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglobal_step:\n",
      "File \u001b[0;32m~/anaconda3/envs/pem-entities/lib/python3.9/site-packages/pytorch_lightning/trainer/training_loop.py:484\u001b[0m, in \u001b[0;36mTrainLoop.run_training_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    482\u001b[0m should_check_val \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 484\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, (batch, is_last_batch) \u001b[39min\u001b[39;00m train_dataloader:\n\u001b[1;32m    486\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mbatch_idx \u001b[39m=\u001b[39m batch_idx\n",
      "File \u001b[0;32m~/anaconda3/envs/pem-entities/lib/python3.9/site-packages/pytorch_lightning/profiler/profilers.py:86\u001b[0m, in \u001b[0;36mBaseProfiler.profile_iterable\u001b[0;34m(self, iterable, action_name)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart(action_name)\n\u001b[0;32m---> 86\u001b[0m value \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(iterator)\n\u001b[1;32m     87\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop(action_name)\n",
      "File \u001b[0;32m~/anaconda3/envs/pem-entities/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:48\u001b[0m, in \u001b[0;36mDataConnector._with_is_last\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     47\u001b[0m last \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(it)\n\u001b[0;32m---> 48\u001b[0m \u001b[39mfor\u001b[39;00m val \u001b[39min\u001b[39;00m it:\n\u001b[1;32m     49\u001b[0m     \u001b[39m# yield last and has next\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     \u001b[39myield\u001b[39;00m last, \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pem-entities/lib/python3.9/site-packages/pytorch_lightning/trainer/supporters.py:470\u001b[0m, in \u001b[0;36mCombinedLoaderIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[39mFetches the next batch from multiple data loaders\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    468\u001b[0m \n\u001b[1;32m    469\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 470\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_next_batch(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloader_iters)\n",
      "File \u001b[0;32m~/anaconda3/envs/pem-entities/lib/python3.9/site-packages/pytorch_lightning/trainer/supporters.py:484\u001b[0m, in \u001b[0;36mCombinedLoaderIterator.request_next_batch\u001b[0;34m(loader_iters)\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    475\u001b[0m \u001b[39mReturn the batch of data from multiple iterators.\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    482\u001b[0m \n\u001b[1;32m    483\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 484\u001b[0m \u001b[39mreturn\u001b[39;00m apply_to_collection(loader_iters, Iterator, \u001b[39mnext\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pem-entities/lib/python3.9/site-packages/pytorch_lightning/utilities/apply_func.py:81\u001b[0m, in \u001b[0;36mapply_to_collection\u001b[0;34m(data, dtype, function, wrong_dtype, *args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, dtype) \u001b[39mand\u001b[39;00m (wrong_dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data, wrong_dtype)):\n\u001b[0;32m---> 81\u001b[0m     \u001b[39mreturn\u001b[39;00m function(data, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     83\u001b[0m \u001b[39m# Recursively apply to collection items\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pem-entities/lib/python3.9/site-packages/torch/utils/data/dataloader.py:435\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> 435\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    436\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/pem-entities/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1085\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1084\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1085\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[0;32m~/anaconda3/envs/pem-entities/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1111\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1110\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1111\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   1112\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/pem-entities/lib/python3.9/site-packages/torch/_utils.py:428\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexc_type(message\u001b[39m=\u001b[39mmsg)\n\u001b[0;32m--> 428\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexc_type(msg)\n",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/20014946/anaconda3/envs/pem-entities/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 198, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/20014946/anaconda3/envs/pem-entities/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"/home/20014946/Documents/Document Adeo/Project/Product_Entity_Matching_Extraction/Attribute_extraction/pdp--product-entity-matching--attributes-extraction/src/attribute_extraction/models/train_utils.py\", line 53, in collate_fun_generator\n    network_input = tokenizer(\n  File \"/home/20014946/anaconda3/envs/pem-entities/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 2488, in __call__\n    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n  File \"/home/20014946/anaconda3/envs/pem-entities/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 2574, in _call_one\n    return self.batch_encode_plus(\n  File \"/home/20014946/anaconda3/envs/pem-entities/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 2765, in batch_encode_plus\n    return self._batch_encode_plus(\n  File \"/home/20014946/anaconda3/envs/pem-entities/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py\", line 429, in _batch_encode_plus\n    encodings = self._tokenizer.encode_batch(\nTypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32m/home/20014946/Documents/Final_Project_PEM/Final_Project_PEM/notebooks/3_training_classification.ipynb Cell 23\u001b[0m in \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/20014946/Documents/Final_Project_PEM/Final_Project_PEM/notebooks/3_training_classification.ipynb#X36sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer \u001b[39m=\u001b[39m pytorch_lightning\u001b[39m.\u001b[39mTrainer(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/20014946/Documents/Final_Project_PEM/Final_Project_PEM/notebooks/3_training_classification.ipynb#X36sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     gpus\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/20014946/Documents/Final_Project_PEM/Final_Project_PEM/notebooks/3_training_classification.ipynb#X36sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     logger\u001b[39m=\u001b[39mmetric_logger,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/20014946/Documents/Final_Project_PEM/Final_Project_PEM/notebooks/3_training_classification.ipynb#X36sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     precision\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m32\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/20014946/Documents/Final_Project_PEM/Final_Project_PEM/notebooks/3_training_classification.ipynb#X36sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/20014946/Documents/Final_Project_PEM/Final_Project_PEM/notebooks/3_training_classification.ipynb#X36sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, train_loader, validation_loader)\n",
      "File \u001b[0;32m~/anaconda3/envs/pem-entities/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:513\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloader, val_dataloaders, datamodule)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpre_dispatch()\n\u001b[1;32m    512\u001b[0m \u001b[39m# dispath `start_training` or `start_testing` or `start_predicting`\u001b[39;00m\n\u001b[0;32m--> 513\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch()\n\u001b[1;32m    515\u001b[0m \u001b[39m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[39;00m\n\u001b[1;32m    516\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_dispatch()\n",
      "File \u001b[0;32m~/anaconda3/envs/pem-entities/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:553\u001b[0m, in \u001b[0;36mTrainer.dispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39mstart_predicting(\u001b[39mself\u001b[39m)\n\u001b[1;32m    552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 553\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49mstart_training(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pem-entities/lib/python3.9/site-packages/pytorch_lightning/accelerators/accelerator.py:74\u001b[0m, in \u001b[0;36mAccelerator.start_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstart_training\u001b[39m(\u001b[39mself\u001b[39m, trainer):\n\u001b[0;32m---> 74\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_type_plugin\u001b[39m.\u001b[39;49mstart_training(trainer)\n",
      "File \u001b[0;32m~/anaconda3/envs/pem-entities/lib/python3.9/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py:111\u001b[0m, in \u001b[0;36mTrainingTypePlugin.start_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstart_training\u001b[39m(\u001b[39mself\u001b[39m, trainer: \u001b[39m'\u001b[39m\u001b[39mTrainer\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m     \u001b[39m# double dispatch to initiate the training loop\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_results \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mrun_train()\n",
      "File \u001b[0;32m~/anaconda3/envs/pem-entities/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:676\u001b[0m, in \u001b[0;36mTrainer.run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_keyboard_interrupt()\n\u001b[1;32m    674\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    675\u001b[0m     \u001b[39m# hook\u001b[39;00m\n\u001b[0;32m--> 676\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_loop\u001b[39m.\u001b[39;49mon_train_end()\n",
      "File \u001b[0;32m~/anaconda3/envs/pem-entities/lib/python3.9/site-packages/pytorch_lightning/trainer/training_loop.py:134\u001b[0m, in \u001b[0;36mTrainLoop.on_train_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39m# trigger checkpoint check. need to temporarily decrease the global step to avoid saving duplicates\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[39m# when a checkpoint was saved at the last step\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 134\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_checkpoint_callback(should_update\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, is_last\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    135\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mglobal_step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    137\u001b[0m \u001b[39m# hook\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pem-entities/lib/python3.9/site-packages/pytorch_lightning/trainer/training_loop.py:164\u001b[0m, in \u001b[0;36mTrainLoop.check_checkpoint_callback\u001b[0;34m(self, should_update, is_last)\u001b[0m\n\u001b[1;32m    161\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mlightning_module\n\u001b[1;32m    163\u001b[0m \u001b[39mfor\u001b[39;00m cb \u001b[39min\u001b[39;00m callbacks:\n\u001b[0;32m--> 164\u001b[0m     cb\u001b[39m.\u001b[39;49mon_validation_end(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer, model)\n",
      "File \u001b[0;32m~/anaconda3/envs/pem-entities/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:212\u001b[0m, in \u001b[0;36mModelCheckpoint.on_validation_end\u001b[0;34m(self, trainer, pl_module)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_validation_end\u001b[39m(\u001b[39mself\u001b[39m, trainer, pl_module):\n\u001b[1;32m    209\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[39m    checkpoints can be saved at the end of the val loop\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave_checkpoint(trainer, pl_module)\n",
      "File \u001b[0;32m~/anaconda3/envs/pem-entities/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:247\u001b[0m, in \u001b[0;36mModelCheckpoint.save_checkpoint\u001b[0;34m(self, trainer, pl_module)\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_add_backward_monitor_support(trainer)\n\u001b[0;32m--> 247\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_monitor_key(trainer)\n\u001b[1;32m    249\u001b[0m \u001b[39m# track epoch when ckpt was last checked\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_global_step_saved \u001b[39m=\u001b[39m global_step\n",
      "File \u001b[0;32m~/anaconda3/envs/pem-entities/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:490\u001b[0m, in \u001b[0;36mModelCheckpoint._validate_monitor_key\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmonitor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_valid_monitor_key(metrics):\n\u001b[1;32m    485\u001b[0m     m \u001b[39m=\u001b[39m (\n\u001b[1;32m    486\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModelCheckpoint(monitor=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmonitor\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m) not found in the returned metrics:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    487\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(metrics\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    488\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mHINT: Did you call self.log(\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmonitor\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, tensor) in the LightningModule?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    489\u001b[0m     )\n\u001b[0;32m--> 490\u001b[0m     \u001b[39mraise\u001b[39;00m MisconfigurationException(m)\n",
      "\u001b[0;31mMisconfigurationException\u001b[0m: ModelCheckpoint(monitor='validation_loss') not found in the returned metrics: ['train_loss', 'train_accuracy_02419', 'train_accuracy_99999', 'train_accuracy_00562', 'train_accuracy_15344', 'train_accuracy_01746']. HINT: Did you call self.log('validation_loss', tensor) in the LightningModule?"
     ]
    }
   ],
   "source": [
    "trainer = pytorch_lightning.Trainer(\n",
    "    gpus=1 if torch.cuda.is_available() else 0,\n",
    "    logger=metric_logger,\n",
    "    callbacks=callbacks,\n",
    "    log_every_n_steps=1,\n",
    "    max_epochs=num_epoch,\n",
    "    precision=16 if torch.cuda.is_available() else 32,\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_loader, validation_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = trainer.test(model, test_loader)\n",
    "test_metrics_sort = [dict(sorted(test_metrics[0].items()))]\n",
    "\n",
    "with open(local_path / \"metrics.json\", \"w\") as f:\n",
    "    json.dump(test_metrics, f, indent=4)\n",
    "\n",
    "with open(local_path / \"metrics_sorted.json\", \"w\") as f:\n",
    "    json.dump(test_metrics_sort, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pem-entities",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
